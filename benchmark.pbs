#!/usr/bin/env bash
#PBS -N HLP 
#PBS -A EnergyOpt_PhaseFreq
#PBS -q debug
#PBS -l select=1:ncpus=1:ngpus=1
#PBS -l walltime=00:15:00
#PBS -l filesystems=home
#PBS -j oe
#PBS -o /home/antdecaro/out.txt
#PBS -e /home/antdecaro/err.txt 

cd $PBS_O_WORKDIR/
echo $(pwd)

set -euo pipefail

RUNS=5
LABEL_SEED=42
LABEL_CLASSES_LIST=(10)
JSON_DIR="data"
BUILD_DIR="build"
LOG_DIR="log"
METRICS_DIR=""
RUN_EXPERIMENT=true
COLLECT_METRICS=""

usage() {
  cat <<EOF
Usage: $0 [options]

Options:
  --collect-metrics {nvidia|amd|intel}  Collect GPU metrics (default: disabled)
  --metrics-dir PATH                    Directory for profiler outputs (default: LOG_DIR/metrics)
  --skip-run                            Skip executing the benchmark binaries; only collect metrics
  -h, --help                            Show this help message
EOF
}

while [[ $# -gt 0 ]]; do
  case "$1" in
    --collect-metrics)
      COLLECT_METRICS="${2:-}"
      shift 2
      ;;
    --metrics-dir)
      METRICS_DIR="${2:-}"
      shift 2
      ;;
    --skip-run)
      RUN_EXPERIMENT=false
      shift
      ;;
    -h|--help)
      usage
      exit 0
      ;;
    *)
      echo "Unknown option: $1" >&2
      usage
      exit 1
      ;;
  esac
done

if [[ -z "$METRICS_DIR" ]]; then
  METRICS_DIR="${LOG_DIR}/metrics"
fi

mkdir -p "$LOG_DIR" "$METRICS_DIR"

timestamp() {
  date '+%Y%m%d-%H%M%S'
}

resolve_exe_name() {
  local exe_basename
  exe_basename=$(basename "$1")
  case "$exe_basename" in
    label_propagation_sycl*) echo "sycl" ;;
    label_propagation_openmp*) echo "openmp" ;;
    label_propagation_kokkos*) echo "kokkos" ;;
    *)
      echo "Unsupported executable name: $exe_basename" >&2
      exit 1
      ;;
  esac
}

collect_metrics_nvidia() {
  local exe_path="$1"
  local json_path="$2"
  local dataset_name="$3"
  local label_classes="$4"
  local seed="$5"
  local metrics_dir="$6"
  local log_file="$7"

  if ! command -v ncu >/dev/null 2>&1; then
    echo "ncu command not found in PATH. Install NVIDIA Nsight Compute CLI to collect metrics." >&2
    return 1
  fi

  local exe_name
  exe_name=$(resolve_exe_name "$exe_path")
  local report_base="${exe_name}_${dataset_name}"

  echo "Collecting NVIDIA metrics with ncu -> ${metrics_dir}/${report_base}.ncu-rep" | tee -a "$log_file"

  local -a base_cmd=(
    "$exe_path"
    --load "$json_path"
    --label-seed "$seed"
    --label-classes "$label_classes"
    --iterations 100
    --tolerance 1e-6
  )

  local -a ncu_cmd=(
    ncu
    -f
    -o "${metrics_dir}/${report_base}"
    # --set full
  )

  if [[ -n "${NCU_EXTRA_ARGS:-}" ]]; then
    # shellcheck disable=SC2206
    ncu_cmd+=(${NCU_EXTRA_ARGS})
  fi

  "${ncu_cmd[@]}" "${base_cmd[@]}" >> "$log_file" 2>&1
}

collect_metrics() {
  local vendor="$1"
  shift || true

  case "$vendor" in
    nvidia)
      collect_metrics_nvidia "$@"
      ;;
    amd|intel)
      echo "Metric collection for $vendor GPUs is not implemented yet." >&2
      ;;
    "")
      ;;
    *)
      echo "Unsupported metrics vendor: $vendor" >&2
      return 1
      ;;
  esac
}

readarray -t JSON_FILES < <(find "$JSON_DIR" -maxdepth 1 -type f -name '*.json' | sort)
if [ ${#JSON_FILES[@]} -eq 0 ]; then
  echo "No JSON files found in $JSON_DIR" >&2
  exit 1
fi

readarray -t EXECUTABLES < <(find "$BUILD_DIR" -maxdepth 1 -type f -executable -name 'label_propagation_*' | sort)
if [ ${#EXECUTABLES[@]} -eq 0 ]; then
  echo "No label_propagation executables found in $BUILD_DIR" >&2
  exit 1
fi

run_experiment() {
  local exe_path="$1"
  local json_path="$2"
  local dataset_name="$3"
  local run_idx="$4"
  local seed="$5"
  local label_classes="$6"
  local log_file="$7"

  local exe_name
  exe_name=$(resolve_exe_name "$exe_path")

  echo "[${count}/${total}] Running $exe_name on $dataset_name (run $run_idx/$RUNS, labels $label_classes)" | tee -a "$log_file"

  "$exe_path" \
    --load "$json_path" \
    --label-seed "$seed" \
    --label-classes "$label_classes" \
    --iterations 100 \
    --workgroup-size 64 >> "$log_file" 2>&1
}

echo "Running benchmarks for ${#EXECUTABLES[@]} implementations on ${#JSON_FILES[@]} datasets across ${#LABEL_CLASSES_LIST[@]} label-class settings"

total=$(( ${#EXECUTABLES[@]} * ${#JSON_FILES[@]} * ${#LABEL_CLASSES_LIST[@]} * RUNS ))
count=0

for exe_path in "${EXECUTABLES[@]}"; do
  exe_name=$(resolve_exe_name "$exe_path")
  for json_path in "${JSON_FILES[@]}"; do
    dataset_name=$(basename "$json_path" .json)

    for label_classes in "${LABEL_CLASSES_LIST[@]}"; do
      log_file="$LOG_DIR/${exe_name}_${dataset_name}_seed${LABEL_SEED}_labels${label_classes}.log"
      : > "$log_file"

      for run_idx in $(seq 1 "$RUNS"); do
        count=$((count + 1))
        if [ "$RUN_EXPERIMENT" = true ]; then
          run_experiment "$exe_path" "$json_path" "$dataset_name" "$run_idx" "$LABEL_SEED" "$label_classes" "$log_file"
        fi
      done

      if [[ -n "$COLLECT_METRICS" ]]; then
        collect_metrics "$COLLECT_METRICS" "$exe_path" "$json_path" "$dataset_name" "$label_classes" "$LABEL_SEED" "$METRICS_DIR" "$log_file"
      fi
    done
  done

done

echo "Benchmarking completed. Logs saved under $LOG_DIR"
